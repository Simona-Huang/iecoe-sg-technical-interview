{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of xgboost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework.Even in this unstructured data it still manages to have a outstanding performance. However, it may not be robust. From the fearture importance display we can see title's length plays a very important role in detecting the fake news. It captures the fact that most fake news are more likely to have longer titles to arouse reader's attention. if this model is deployed ,overtime , the fake news maker may learn to avoid being caught just by giving the news shorter title. \n",
    "\n",
    "![title](feature_importance.png)\n",
    "\n",
    "Through the feature importances ,we can see the two engineered features together have an impact on the classification. It verifies the speculation that fake news tend to have more adjectives depicting abundant details to fool people and true news tends to have more objective tone than fake news. \n",
    "However it is very fast for training nearly in one second to train 3000+ texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of text_CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neutral network has had remarkable records of the accuracy in image recognition. Inspired by this, the attempts to apply it in text related tasks , such as classification and machine translation,also have been productive. \n",
    "The architecture of the text_CNN I implemented is from the paper Convolutional Neural Networks for Sentence Classification by Yoon Kim. It contains an embedding layer and 3 filters with different kernels which are equivelant to extract information from different n_grams,a maxpooling layer and is ended with an dropout layer and dense layer with sigmoid activaton function to do the binary classification.The following is the summary of the model:\n",
    "![title](text_cnn_summary.png)\n",
    "This structure processes all the words parallelly , different from RNN sequential models, which can learn sequential information but suffer from gradiant vainish proplem as the sequence goes too long. In this case, the maximum length is 5000+,and the objective is to do classification ,not machine translation, or captioning, sequential information may be not that critical, thus CNN is a better structure to capture all the information to make the decision. The learning curve demonstrates that this architecture is very fast  to learn the task. The loss dropped quickly in the second epoch. \n",
    "The test f1 score is 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of BERT is the tranformer which has attention mechanism. In addition to the traditional transformer, BERT has bidirectional attention, which can utilize information both from right and left side. BERT models return a map with 3 important keys: pooled_output, sequence_output, encoder_outputs. pooled_output represents each input sequence as a whole, so I add a dense layer on this pooled_output layer to do the classification. following is the model summary:\n",
    "![title](summary_bert.png)\n",
    "\n",
    "However,compared with text_CNN,BERT has much more parameters and is slow in training, even it also achieves test f1 score 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of models and conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there is no need to tune the hyperparamter by this project, I didn't seperate validation set for most of the models. \n",
    "![title](models.png)\n",
    "\n",
    "As tree model picks length of title and type of news as the most important features ,other than the text itself, this makes NN models more favorable. \n",
    "\n",
    "Among NN models , textCNN has far less parameters than the BERT but has the same performance with BERT, we can see from the table even with larger training datasets, textCNN still has less training time,  so i will finally choose textCNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
